# AI Assistant Rule Configurations for SpeechDown

*Status*: Finished
*Date*: 2025-06-09
*ChatGPT Chat: [AI Assistant Rules Research](https://chatgpt.com/c/6846b2d1-d59c-800f-a336-7d9e9a259925)

## Problem

SpeechDown aims to harness AI coding assistants to boost development productivity, but each tool must be guided to **strictly follow our project’s Domain-Driven Design (DDD) and Ports & Adapters architecture (ADR 008)**, naming conventions, testing standards, and documentation practices. Unconstrained AI suggestions could violate layering (e.g. placing logic in the wrong module), use inconsistent names, omit tests, or introduce insecure code. We need a way to encode our architectural rules (from ADRs) into each assistant’s configuration so that AI-generated code aligns with:

* **Layered DDD architecture** – e.g. domain logic stays in `domain/`, adapters in `infrastructure/adapters/`, etc., with no cross-layer imports except as allowed.
* **Naming conventions** – e.g. interfaces end in *Port*, implementations end in *Adapter*, services end in *Service*.
* **Testing and docs standards** – e.g. all changes require unit tests (using pytest, following Arrange-Act-Assert style per ADR 004), and significant features should include a Design Doc (per ADR 010).
* **Security and quality** – e.g. no credentials in code, no use of deprecated/vulnerable dependencies, and code passes linting (ruff), type checks (mypy), and our CI pipeline.

In summary, we need to **establish repository-specific “AI guardrails”** for each AI assistant (Google Jules, GitHub Copilot, OpenAI Codex, Anthropic Claude, and Cursor) to ensure their contributions comply with our architecture and coding standards. The challenge is that each platform supports different rule formats and capabilities to constrain AI behavior. We must identify how to define rules/config files for each tool and integrate those into our repo and CI workflow, without relying on manual oversight or sacrificing the benefits of AI assistance.

## Approach

We researched official docs and community resources for each AI coding assistant to find how they support project-specific rules or configuration files. Our approach:

1. **Identify Config File Formats & Locations:** For each assistant, determine if it uses special files (e.g. config YAML/MD files in the repo, hidden directories, etc.) to influence its suggestions or actions. We focused on known mechanisms like GitHub Copilot’s instructions file, OpenAI Codex’s `AGENTS.md` convention, Claude’s `CLAUDE.md`, Cursor’s rules files, and any pattern for Google’s Jules. We noted naming conventions and directory placement for these files to ensure they are discovered by the tools.

2. **Map Rules to Architecture & Standards:** Using SpeechDown’s ADR 008 (architecture) and ADR 004 (testing), etc., we drafted rule content that would instruct an AI about our layered design, allowed dependencies, naming requirements, and test/documentation obligations. We aimed for concise, natural-language rules since most assistants expect plain English instructions. For each tool, we planned how to phrase rules (e.g. “All database access must go through an Adapter in `infrastructure/adapters/`…”). Where possible, we favored **fail-safe instructions** (“the AI *must* run all tests and linters before finalizing changes”) given some agents can execute commands.

3. **Review Security Features:** We examined how each assistant handles secrets or unsafe code. This included checking if they have built-in secret detection or content filters (e.g. Copilot’s filters to avoid suggesting credentials or large verbatim code) and whether we can restrict certain AI actions (Claude’s settings to deny running dangerous commands, Copilot’s content exclusion of paths, etc.). We compiled the security measures (like path redaction or dependency update checks) each platform supports out-of-the-box or via config.

4. **Plan CI/CD Integration:** We explored ways to integrate these rule files and AI tools into our development workflow. This involved looking at GitHub Actions or CLI tools: e.g. **Anthropic Claude’s GitHub Action** for PRs, the possibility of automating Codex agent tasks (via ChatGPT or an API/CLI), and community tools for Jules integration (e.g. the `google-jules-workflow` scripts). Our goal was to ensure the rule files are not only present but also leveraged in automation – for instance, an AI-driven code review step that respects our rules, or a CI check that AI-suggested PRs haven’t violated the rules.

We prioritized using **built-in configuration mechanisms** for each assistant (to avoid complex custom tooling) and ensuring these configurations reside in the repository (`docs/` or config files) for transparency and version control. Below, we detail our findings per assistant, including the specific rule file formats, examples of rules to enforce our architecture, and how each can be secured and integrated into our pipelines.

## Findings

### Google Jules (Asynchronous Coding Agent)

**Rule File Format & Location:** At present, Google **Jules does not use a formal repository config file** like some other agents do. Jules is invoked by writing a task prompt (via the Jules UI or by labeling a GitHub issue) and it autonomously clones the repo into a secure VM. There is no officially documented “Jules.md” or config that Jules automatically reads. However, guidance can be given *in the prompt or issue description*. For example, when assigning a task to Jules, we can prepend a summary of project conventions (architecture layers, naming) so Jules plans accordingly. Some users have experimented with creating a `jules.md` in the repo, but it’s unclear if Jules consistently reads it (anecdotal evidence suggests Jules might update a `jules.md` with notes during tasks, but this is not confirmed in official docs).

**Enforcing DDD Architecture:** Without a dedicated config file, the recommended approach is to **include architectural instructions in each Jules task prompt**. For instance, if asking Jules to implement a feature, we would start the prompt with a reminder of our architecture: *“This repo follows a layered Domain-Driven Design (see ADR 008). Please confine business logic to `src/speechdown/domain/`, use interfaces in `application/ports/` for external interactions, and implement them in `infrastructure/adapters/`. Adhere to naming conventions (e.g. `TranscriptionPort`, `XYZAdapter`).”* Jules’ planning step will list files to change; by providing these hints, we expect Jules to choose the correct locations (e.g. putting new database code in an adapter, not directly in domain code). The **Ports & Adapters pattern** can be explicitly mentioned so the AI agent understands the role of each folder. Since Jules presents a plan for approval, the developer can verify it respected the boundaries before execution.

**Restricting Changes & Naming:** Jules’ plan mechanism inherently restricts changes to the files it lists for the task. To guide it, we can specify in the prompt which directories are in scope: e.g. *“Modify only infrastructure adapter code to add this feature; do not alter domain entities.”* This acts as a soft restriction. Jules doesn’t support a fine-grained ignore list in the repo, but the user remains in control by reviewing the diff it proposes. We should also encode naming rules in the prompt (as above). Jules typically follows instructions well if clearly given, but lacks persistent memory of project style across tasks (aside from what it “learns” per session). Thus, consistency must be reinforced each time or via documentation that Jules can read (like referencing our ADR docs in the prompt).

**Testing & Documentation Compliance:** Jules can run tests as part of its task if instructed. It’s good practice to ask Jules to run our test suite after code changes (Jules uses the cloud VM to execute commands). We can tell Jules: *“After coding, run `make ci` (which runs linters, mypy, and tests) and ensure all checks pass.”* Jules will then include the test outputs in its report. It commits changes only after completing tasks and can even open a PR automatically. We must explicitly require tests – e.g. *“Include unit tests for any new functionality (tests are under `tests/` directory).”* Jules is capable of writing tests (it lists “Writing tests” as one of its capabilities). For documentation, we might instruct: *“Update relevant documentation (e.g. README or ADRs) if behavior changes.”* Jules doesn’t inherently know about our ADR process, so we must prompt it if a design decision warrants an ADR or Design Doc. In practice, this may be beyond Jules’ current capabilities (writing an ADR may require a separate task), but at least it can be directed to update docstrings or README content as part of a task.

**Security Measures:** Jules operates in a **secure, isolated environment** – Google emphasizes that your code stays in a private VM and **Jules does not train on your repo data**. This mitigates the risk of leaking secrets during the AI’s operation. Jules likely has internal guardrails (being powered by Google’s Gemini model) to avoid outputting sensitive info, but specifics aren’t public. We should **not rely solely on the AI**: for example, ensure no plaintext secrets are in the repo that an AI could accidentally expose. Unlike Copilot, there’s no user-facing “exclusion list” for Jules. On the plus side, Jules will bump dependencies on request – to enforce avoidance of insecure dependencies, we can periodically assign Jules a task like “Upgrade outdated packages to the latest safe versions.” Jules will then handle version bumps and run tests. This can be part of our maintenance workflow (with us verifying the changelog for security fixes).

**Integration into Workflow:** We can integrate Jules in two ways: **manually via issues** or using automation scripts. Manually, we create a GitHub issue, add the label (e.g. “assign-to-jules”), and include all instructions. Jules will pick it up and create a branch/PR with changes. To make this smoother and consistent: (a) create an issue template that already includes a section reminding the author to specify architecture and testing requirements for Jules, so nothing is missed; (b) use Jules’ plan review to catch any rule violations (if Jules proposes touching a forbidden layer, we reject or adjust the prompt). For further automation, there is a community tool called **Jules Workflow Optimizer** that can manage Jules and Copilot interactions via CLI and GitHub Actions. For example, after Jules opens a PR, it can auto-assign **Copilot as a reviewer**. We could incorporate a job in our CI to run `jules-pr assign-copilot` which triggers Copilot to review Jules’ PR for issues (ensuring a second AI check for compliance). While experimental, this shows how Jules can fit into CI: Jules does the coding in its VM, then our GitHub Actions run tests (which they already do), and optionally an AI review. In summary, Jules’ rule enforcement is mostly at prompt-time and human review time, with no persistent config file; therefore, our integration plan relies on structured prompt templates and review checkpoints rather than file-based rules for Jules.

### GitHub Copilot (Chat & Code Completion)

**Rule File Format & Location:** GitHub Copilot supports repository-specific customization primarily through a markdown file: **`.github/copilot-instructions.md`**. This file lives in the repo (inside `.github/`) and contains natural language instructions that Copilot Chat will automatically incorporate into its responses. These instructions are added as hidden context to every Copilot chat query for that repo. For example, our `copilot-instructions.md` might say: *“This project uses a Ports & Adapters architecture – do not put business logic in the UI or infrastructure layer. Follow naming conventions from ADR 008 (interfaces end in Port, implementations end in Adapter). Ensure all new code has corresponding pytest unit tests and follows PEP 8 and our ruff config.”* Such a file gives Copilot a baseline understanding of the project’s norms. In addition, Copilot (in VS Code) allows **Prompt Files** (`*.prompt.md`) that store reusable prompt snippets for common tasks. For instance, we could create `API security review.prompt.md` containing a checklist for secure API design; a developer can quickly inject this into a Copilot Chat session when asking for a security review. Prompt files reside in the repository (typically we’d keep them under a docs folder or in `.github/` as well) and are accessible via the Copilot UI in VS Code (currently this feature is VS Code-specific). They serve as **task-specific rule presets**. Together, the instructions file (for always-on guidelines) and prompt files (for on-demand scenarios) form the primary repo-specific config for Copilot.

**Enforcing Architecture & Naming:** In our **`.github/copilot-instructions.md`**, we will explicitly describe the SpeechDown architecture. Copilot’s documentation suggests writing these instructions in a **natural, descriptive manner** (not just a list of rules, but as explanatory text). For example: *“SpeechDown is organized in four layers (Domain, Application, Infrastructure, Presentation). Each layer has specific responsibilities. When writing code, ensure that domain logic stays in the `speechdown/domain` module and does not depend on other layers. Use interfaces (called Ports) under `application/ports` for any external interactions, and implement them in `infrastructure/adapters` (Adapters). Name new interfaces with the suffix **Port** and adapters with **Adapter** (e.g. `TranscriptionPort`, `FileTranscriptionAdapter`). Follow our naming pattern for services (UseCase services end in **Service**).”* By including such specifics, we equip Copilot with context so that, for instance, if a developer asks in Copilot Chat “Generate a new service to handle transcription caching,” the AI will (ideally) suggest creating it in the `application/services` module and perhaps name it `TranscriptionCacheService` rather than something off-pattern. The instructions file can also mention ADR references: e.g. *“Refer to ADR-008 in `docs/adrs/current/008_project_architecture.md` for architecture guidelines.”* While the AI might not literally open that file, mentioning it reinforces the existence of a formal standard.

For naming conventions, we list them out clearly (as above). Copilot’s model will take that into account when proposing class or function names. It may not always perfectly comply, but testing has shown that these custom instructions do bias its outputs in the right direction. One limitation: the instructions file currently influences **Copilot Chat** more than the raw code completion suggestions. Inline code completions (the “ghost text” while typing) rely mostly on immediate code context and broader training. They might not always heed high-level repo rules, especially if the file in question isn’t open. However, Copilot is increasingly context-aware; having the file in `.github/` means it’s likely in the index and could be considered. In any case, developers using Copilot Chat to generate larger chunks or discuss implementation will definitely benefit from the injected rules.

**Restricting Changes to Directories:** Copilot itself won’t arbitrarily create or edit files without the user’s action – it responds to the file you are editing or the prompt you give. Thus, “restricting” changes is mainly about guiding where new code should go. Our instructions can say, for example, *“All database access code should reside in `infrastructure/adapters/` and nowhere else.”* If a developer asks Copilot to add some database logic in a service, the hope is the AI might suggest using a port or remind the developer of the separation (in practice, this might vary). For stronger guarantees, we rely on linting/CI (discussed later) to catch violations. But Copilot’s suggestions will at least align with the idea that, e.g., it shouldn’t suggest importing an infrastructure class directly into a domain class if our instructions forbid cross-layer dependency. In summary, Copilot can be **nudged** to keep changes confined to intended layers, but the developer remains responsible for accepting or rejecting suggestions.

**Testing & Documentation Compliance:** We will also encode testing requirements in `copilot-instructions.md`. For example: *“All code must have unit tests. We use **pytest** (ADR-004); follow the Arrange-Act-Assert pattern and place tests under `tests/` with a similar module structure. Copilot should output tests for any new functionality whenever possible.”* With this in context, when using Copilot Chat, a developer could ask, “Write tests for the new adapter I just wrote,” and Copilot will know that we prefer pytest and a certain style. Even without being asked explicitly, Copilot might proactively include a test file suggestion if prompted for a full implementation of a feature (some users report Copilot Chat will sometimes say “I’d also add a test file…” if instructed well). We also instruct about documentation: *“For significant changes, update relevant docs. We maintain Design Docs in `docs/design/current/` for new features (see ADR-010), and any major decision changes should be reflected in an ADR in `docs/adrs/`. Copilot should not ignore documentation updates.”* While Copilot won’t on its own open our docs and append text, this serves as a reminder to the developer (via the AI’s suggestions) not to forget docs. In practice, Copilot might include comments or suggest an update to README if we mention it here.

**Security and Secret Handling:** GitHub Copilot has a few built-in security guardrails. Notably, it **filters out suggestions that appear to contain secrets or large chunks of copyrighted code**. For example, if a completion matches a known public code snippet above a certain length, Copilot will refuse to suggest it. Also, by organizational policy, we can **block Copilot from accessing certain files** – specifically, GitHub allows repository owners to set **“Content Exclusions.”** This is configured in repo settings (not via a file in the repo) where we list file path patterns that Copilot should ignore. As a best practice, we will add common secret files to that exclusion (e.g. any `.env` or secret config files, although ideally we don’t commit those at all). For instance, we can exclude `"*/.env"` or `"secrets.*"` so that if such files exist, Copilot won’t train on them or use them in suggestions. Our repository’s `.gitignore` already prevents committing secrets, but the extra layer ensures Copilot never even sees placeholder API keys in config. Copilot’s **output filters** also will prevent it from showing something that looks like a credential (it has heuristics for patterns like AWS keys). As for insecure code patterns (like SQL injection or use of outdated libraries), Copilot doesn’t have specialized filters there – it relies on the user prompt and its training. However, our custom instructions can include security guidelines: e.g. *“When generating code, avoid use of deprecated libraries or insecure functions. Prefer parameterized queries for SQL, use prepared statements, etc.”* We also created a prompt file `Security.audit.prompt.md` with a list of security checks (authentication, input validation, etc.), so a developer can ask, “Copilot, review this code for security issues” and the AI will apply those points. This isn’t automated in CI, but provides a convenient way to do AI-assisted security reviews during development.

**CI/CD Integration:** GitHub Copilot is primarily a developer-side tool, so it doesn’t run in CI. However, GitHub has introduced **Copilot for Pull Requests** – this can automatically generate **PR summaries and even do AI-powered code review** on PRs. We plan to enable Copilot’s PR review for our repo (it’s available for Copilot for Business and can be configured to auto-review PRs). With that, each PR will get an “Assistant” review comment from Copilot highlighting potential issues or TODOs. This review should respect our repository instructions to some extent (though GitHub’s documentation isn’t explicit if the same `copilot-instructions.md` is fed into the PR reviewer AI, it likely is considered). This provides a safety net: if a contributor (human or AI) violates a rule, Copilot’s review might catch it (for example, “I notice this new class doesn’t follow the naming convention” or “No tests were provided for this change”). We won’t rely on it alone, but it complements human reviews.

In CI, aside from enabling the Copilot PR reviewer, we ensure our **lint/type/test suite (ruff + mypy + pytest)** runs on every push – this will catch many issues (syntax, style, type errors). The AI’s suggestions are subject to the same checks as human code. If Copilot suggested something that doesn’t pass lint, the CI will fail, prompting a fix. Over time, Copilot will learn from minor corrections (it adapts per session) – e.g. if it suggests code that doesn’t pass ruff and we consistently fix it (or instruct it in chat to format properly), it should align better with our style (our instructions file explicitly mentions using Black/ruff formatting rules).

Finally, maintaining the **`.github/copilot-instructions.md`** file is important: whenever we update our ADRs or standards, we should update this file. Integrating that into our process, we’ll treat changes to architecture or testing policy as needing an update to the Copilot instructions (this can be a checklist item in ADR implementation). This way, the next developer to use Copilot gets up-to-date guidance.

### OpenAI Codex (ChatGPT “Code” Agent)

**Rule File Format & Location:** OpenAI Codex (the new agent available through ChatGPT Plus/Enterprise as of mid-2025) uses a special markdown file convention: **`AGENTS.md`**. We learned that **Codex automatically scans for any `AGENTS.md` files in the repository and uses their content to guide its behavior**. These files can be placed at the repo root or in subdirectories; Codex merges the instructions in a hierarchical manner. Specifically, an `AGENTS.md` affects the directory it’s in and all subdirectories (its “scope”). If multiple are present, the more deeply nested ones override higher-level ones for that sub-tree. This means we can have a global `AGENTS.md` with overall rules, and, if needed, folder-specific `AGENTS.md` files for fine-tuning (e.g. one in `tests/` with testing-related notes, or one in `scripts/` if we had one-off guidelines there). The file format is simple: it’s a free-form Markdown text file – essentially like writing guidelines to a junior developer. We do not need YAML front-matter or special syntax; Codex will parse the content. The **AGENTS.md spec** published by OpenAI indicates we should include things like how to run tests, coding conventions, etc., and that Codex **must follow these instructions** when performing tasks.

For SpeechDown, we will create a primary **`AGENTS.md` at the root** of the repository. This will serve as the master playbook for the AI agent. If we find certain directories need extra detail, we can add additional files (for now, one global file seems sufficient).

**Enforcing Architecture & Running Checks:** In our `AGENTS.md`, we will thoroughly document the expected project structure and process. A likely structure for the file:

* **Project Overview & Architecture:** A section describing the DDD layers and their content. For example:

  **Architecture:** “This project follows a Hexagonal Architecture (Ports & Adapters). **Domain layer** (`speechdown/domain`) contains core business logic and is independent of external systems. **Application layer** (`speechdown/application`) defines interfaces (ports) in `ports/` and orchestrates use-cases in `services/`. **Infrastructure layer** (`speechdown/infrastructure`) provides implementations (adapters) of the ports – e.g., file or API implementations. **Presentation layer** (`speechdown/presentation`) handles CLI and output formatting. Each layer can only depend on the layers inward (e.g. infrastructure depends on application/domain, domain depends on nothing external).”

  We will then list **naming rules**: “Name interfaces as `SomethingPort` and adapters as `SomethingAdapter`. For example, `AudioTranscriberPort` in application/ports should have an implementation `WhisperAudioTranscriberAdapter` in infrastructure/adapters. Follow this naming for consistency.”

  Codex will use this information when deciding where to create new files or functions. The spec explicitly says it must obey instructions about code style/structure for any file it touches, so if we task Codex with adding a new feature, it will consult these rules to, say, create a new Port interface rather than hard-coding an integration.

* **Build/Testing Commands:** We’ll instruct Codex how to run our CI checks. Very importantly, the `AGENTS.md` can include **programmatic checks that the agent must execute**. We will add a section:

  **Testing & Linting:** “Before finalizing any code changes, run all checks: `make ci` (which runs linting, mypy type checks, and unit tests). Ensure this command exits with 0 status. If any test or check fails, fix the issues. All tests (in `tests/`) must pass, and coverage should remain high. Do not consider the task done until `make ci` succeeds.”

  According to OpenAI’s rules, if such instructions are present, Codex **MUST run them and verify the result after making changes**. This is powerful: it means the agent will not consider its job finished (and will not propose a PR) until our suite passes. In practice, the ChatGPT interface for Codex shows the agent’s “terminal output” – we will actually see it installing dependencies, running pytest, etc., then adjusting code if something fails. This essentially enforces our coding standards automatically. For example, if Codex introduces a lint error, ruff will flag it, and Codex will correct the formatting. If it misses a case and a test fails, Codex will iterate until the test passes, as long as the tests themselves are correct.

* **Project Conventions:** Additional notes like “Use f-strings for string formatting; adhere to PEP 8; use type hints (this project is type-checked with mypy in strict mode).” We have mypy in our pipeline, so stating this ensures Codex runs mypy (via `make ci`) and fixes type errors rather than ignoring them. We can also mention “We use `ruff` for linting, with a line length of 100, so format accordingly.” Codex, being an agent, might even auto-run `ruff --fix` if it knows the command, but if not, any lint issues will be caught by `make ci`.

* **Documentation & ADRs:** We will instruct: “If your changes involve a new significant decision or feature, you must create or update a Design Doc in `docs/design/current/` (per ADR-010) and possibly log an ADR in `docs/adrs/` if it’s an architectural decision. Use the date-based filename format for any new doc. Also update `README.md` or help text for CLI if applicable.” Now, Codex’s ability to write prose like an ADR is untested, but it might draft something if asked. At minimum, including this in `AGENTS.md` means the agent will be aware that such documentation exists. It could then either prompt the user that a Design Doc is needed or attempt to create one. Since ADR-003 standardized ADR content generation with AI prompts, it’s conceivable we could have Codex assist in writing ADRs too, following that prompt. For now, the key is to put the idea in the agent’s head that code changes aren’t complete until docs are considered.

**Restricting Scope of Changes:** With Codex, scope restriction is explicit: the agent will only work on the task we assign. If we formulate the task like “Refactor the caching logic in the infrastructure layer to use the new interface,” Codex will focus on those files. The presence of `AGENTS.md` rules like *“For every file you touch, obey the instructions for that scope”* ensures it won’t wander off modifying unrelated parts. Moreover, if we have multiple `AGENTS.md`, it localizes rules. For example, we might add a `speechdown/domain/AGENTS.md` that says “No external imports here (only Python stdlib and domain code).” Then if Codex is editing a domain file, it will know not to, say, import an HTTP library directly (it should go through a port). If it violates that, it’s breaking its own spec. Based on OpenAI’s description, Codex’s internal policy gives high priority to `AGENTS.md` content, second only to direct user instructions. That means if our *task prompt* explicitly told it to do something conflicting with `AGENTS.md`, our prompt wins. But we, of course, will not intentionally ask it to violate architecture. So the `AGENTS.md` acts as a steadfast rulebook in normal operation.

**Security Practices:** By design, Codex runs in a **cloud sandbox environment** for each task. It can execute commands, access the repository, and even browse the internet if enabled. Security-wise, this sandbox is isolated from production or any private infrastructure – it’s akin to a temporary VM similar to Codespaces. It provides “verifiable evidence” of actions including logs and test results, which increases transparency. For secret handling: if our repo had secrets (again, ideally it doesn’t), Codex could potentially see them, but OpenAI claims to prioritize security. Likely, like ChatGPT, it will refuse to output long secrets verbatim or anything that looks like a key – but we won’t test that. We will simply ensure secrets aren’t there or are excluded via `.gitignore`. We can add in `AGENTS.md`: “Never expose credentials or API keys in outputs.” This serves as a reminder and should the agent somehow consider printing a token (perhaps in logs), it might redact or omit it due to OpenAI’s own content policy. Regarding **insecure dependencies**, Codex doesn’t automatically know CVEs, but we can instruct it to use `pip audit` or check our dependency file. If we include a rule like *“Always upgrade to the latest patch version of dependencies if you are modifying that area”*, Codex might do it (it can edit `pyproject.toml` or `requirements` if told to bump versions). We saw Jules can bump deps; Codex similarly could if asked. But to be safe, we’ll continue using Dependabot for systematic updates. Codex’s internet access could be used to fetch docs or even check if a library is the latest – in fact, if a test fails due to a deprecation, Codex might search for the error online and adjust code accordingly.

**Integration into CI/CD:** OpenAI Codex is accessed via ChatGPT’s interface or an API/CLI (OpenAI released a CLI `openai/codex` for terminal-driven use). It’s not a persistent agent in CI; rather, it’s used on-demand for coding tasks. To integrate it, we might use it during development (like a super-charged Copilot when we need a large refactor or multi-file change). We could also experiment with an **automated trigger**: e.g. a GitHub Action that, when a specific label is applied (“codex-task”), calls the OpenAI API to run Codex on our repo with a given prompt. However, this is non-trivial and currently experimental. A safer integration is: use Codex for complex changes locally, then rely on our normal CI to test those changes. The real integration work for Codex is maintaining `AGENTS.md` and ensuring it’s up to date with our latest practices. We will treat `AGENTS.md` as a living document: when our test process changes or we adopt a new tool, update that file. Also, because Codex can create PRs with descriptions, we might integrate a step where after Codex completes a task and we get a PR, we run CI and maybe have Copilot/Claude review it. Essentially, Codex would act as an automated contributor that follows our rules. Our job is to ensure `AGENTS.md` is comprehensive and correct. The bottom line is that **Codex will be a powerful ally in enforcing standards** – it literally won’t finish its job until it meets the criteria we’ve listed (tests passing, etc.). This greatly reduces the risk of it introducing broken or non-compliant code into `main`.

### Claude Code (Anthropic Claude AI Assistant)

**Rule File Format & Location:** Anthropic’s Claude Code agent uses a concept similar to Codex’s config, but in a simpler form: a **`CLAUDE.md`** file. Claude will automatically include the contents of any `CLAUDE.md` in the working directory (and some parent directories) into its context when you start an AI coding session. Typically, you place one at the root of your project (where you invoke Claude). This file is free-form Markdown (no special front-matter needed) and is meant to capture project-specific tips, commands, and guidelines. The official guidance suggests documenting things like common build commands, code style rules, testing instructions, and any project “etiquette” or quirks. We will create a `CLAUDE.md` at the root of SpeechDown containing our rules. Additionally, Claude Code supports multiple such files if you run it in subdirectories: it will pull in the root one and any `CLAUDE.md` in the sub-folder you’re working on, as well as a global one in the user’s home directory if present. For our purposes, one file at root suffices, but if SpeechDown grows into a monorepo or has clearly separate components, we could add context-specific Claude files in those sub-folders.

**Project Rules & Architecture in CLAUDE.md:** We will fill `CLAUDE.md` with content similar to what we put in Codex’s `AGENTS.md` (there’s some overlap, and we can reuse text). Key sections:

* **Architecture & Naming:** A succinct description of layers and naming (Claude’s context window is large, but we should keep it concise for readability). For example: *“**Architecture**: This project follows Domain-Driven Design. Core logic is in `src/speechdown/domain/` (no external imports here). Use cases and interfaces are in `application/` (see `application/ports/` for abstract interfaces). External integrations implemented in `infrastructure/adapters/`. Presentation (CLI) in `presentation/`. All dependencies point inward (e.g., infrastructure depends on application and domain, but domain cannot depend on infrastructure). **Naming**: Interfaces end with Port, e.g. `TranscriptionPort`; Adapters end with Adapter, e.g. `WhisperTranscriberAdapter`. Services end with Service.”*

* **Coding Style & Linting:** We will note that we use Python 3.12+, with type hints everywhere (the project is PEP 484 compliant). Also: *“We use the **ruff** linter (which auto-formats code) with 100 char lines, so format accordingly. Run `ruff --fix` before commit. We use `mypy --strict` for type checking; ensure no type errors.”* Claude tends to obey explicit instructions like these – it will often apologize and fix code if it realizes it violates a “YOU MUST” rule in the context. In fact, Anthropic suggests using emphatic tone for important rules. We might literally include lines like: “**IMPORTANT**: You must run `make ci` and confirm it passes before finalizing any code.” While using all-caps in these files isn’t necessary, Anthropic’s engineers note it can improve model adherence. We’ll likely incorporate that trick for critical items (like running tests, respecting architecture boundaries).

* **Build/Run/Test Commands:** A section listing our common commands: e.g. *“**Commands**: `make ci` – run lint, mypy, tests. `make test` – run unit tests, `make test-integration` for integration tests. `sd` – CLI entry point (installed via console script). `pytest -k <pattern>` for focused tests. Use these commands when appropriate.”* By including these, Claude can directly use them. Claude Code actually allows the AI to execute commands in the terminal during a session (similar to Codex). If we’re in an interactive session and ask Claude to fix a failing test, it might on its own run `pytest` to see what’s failing. Having the exact command ensures it uses the right one (e.g. `make ci` rather than some guess).

* **Testing & ADRs:** We’ll add: *“All new features must include unit tests (located in `tests/unit/` mirroring the module). Follow Arrange-Act-Assert pattern in tests. Use pytest features (fixtures, parametrize) instead of writing loops.”* Also: *“If a new feature is significant, update `docs/design/current/` with a design doc (per ADR-010) and possibly create an ADR in `docs/adrs/` for any architecture decision.”* Again, this educates Claude on our process.

One advantage: **Claude Code has a conversational memory** and a REPL-like interface. As we work, we can refine these rules. For instance, if during a session we realize Claude did something slightly off, we can hit `#` (special command) to append an instruction to `CLAUDE.md` on the fly. For example, if it named something incorrectly, we add “When naming adapters, always include the word Adapter at end.” That persists for future sessions. We should periodically review and commit changes to `CLAUDE.md` so that they’re version-controlled. Anthropics engineers even mention running the `CLAUDE.md` through a prompt improver to tighten it up; we can consider that as a meta-step – using the AI to refine its own rules.

**Security Measures (Claude):** Claude is known for its focus on safety (Anthropic’s “Constitutional AI”). This means Claude might be less likely to produce insecure code or might warn about it. Our rules can explicitly cover security: e.g. *“Never print sensitive info (tokens, credentials). Sanitize any file paths or personal data before outputting logs.”* Claude is pretty good about not exposing the user’s file system paths or personal details unless needed; in fact, when integrated via GitHub Actions, it operates on GitHub’s runners so secrets are not exposed to it directly (it would only see them if they were in code or logs). We also have control via **Claude’s settings.json** to limit its actions. For example, in `.claude/settings.json` we can set permissions to **deny certain operations**. If we were concerned about, say, Claude executing network calls or destructive commands, we could add: `"deny": ["Bash(rm *)", "Bash(curl *)"]` etc. The example in Anthropic docs shows you can whitelist allowed commands and implicitly deny others. We will likely permit only our standard dev commands (install, test, etc.) and no deployment or external network touches. This prevents any accidental dangerous operations. Also, Claude Code by default doesn’t have access to the internet unless configured, and in enterprise settings it can run via a proxy with audited access.

Claude’s **security features** also include privacy: code stays on the developer’s machine or CI runner (for the GitHub Action, it runs on GitHub’s hosted runner, not Anthropic’s servers except for the model API calls). Claude respects our rules too – if we have a rule about avoiding insecure dependencies, Claude won’t knowingly introduce one. It may even proactively suggest safer alternatives; for instance, if we were using an outdated library, Claude might suggest updating it as part of a task (Anthropic’s blog notes using Claude to handle upgrades and dependency bumps too).

**Integration into Workflow:** We foresee two main integration points for Claude: **developer’s local environment** and **GitHub Action for PRs**.

Locally, a developer can use Claude via the command-line tool or an IDE integration. With `CLAUDE.md` in place, every time they invoke Claude on SpeechDown, the AI will read our guidelines first. This is seamless – nothing extra needs to be done except keeping the file accurate. If a developer says “Claude, add a new command to the CLI to export transcripts to CSV,” Claude will follow our structure (it knows CLI lives in `presentation/cli/commands` perhaps from the rules) and produce code accordingly. The developer can iterate with Claude, with the agent executing tests, etc., as needed until the change is ready.

On CI/CD, Anthropic provides a **Claude Code GitHub Action** (currently beta) that can be summoned by mentioning `@claude` on a PR or issue. We plan to experiment with this for code reviews or even automated PR generation. For example, a workflow could be: developer opens a draft PR with some outline, comments `@claude summarize` or `@claude help fix tests` – and the Claude Action will respond. It is stated that Claude’s GH Action “follows your CLAUDE.md guidelines and existing code patterns”, which is crucial – it means even in an automated context, our rules file is in effect. A possible use: create a GitHub Action that triggers Claude to attempt a fix when CI fails (a bit futuristic, but conceivable: Claude sees a failing test and tries to commit a fix). More realistically, we might use it to **automate documentation or refactoring tasks** by opening an issue and letting Claude create a PR. Those PRs would be governed by our rules.

To integrate safely, we would store an Anthropic API key in GitHub Secrets (for the action) and configure the action to have only limited repo permissions. Given Claude can potentially write code, we’d use branch protections to ensure human review still occurs. The plan for now is to integrate Claude in an assistive role – e.g., have it comment on PRs with improvements. This is low-risk and uses our `CLAUDE.md` as reference.

Maintaining the `CLAUDE.md` is similar to `AGENTS.md` – it should evolve with the project. Because Claude’s model is very compliance-oriented (often following instructions closely), any misinformation or omission in `CLAUDE.md` could lead it astray. So if, for instance, we add a new layer or rename a module, we must update `CLAUDE.md` immediately. This will be part of our development checklist whenever architecture changes.

### Cursor (AI-Powered Code Editor)

**Rule File Format & Location:** Cursor is an AI-enabled IDE (with its own code editor) that allows user-defined **“Rules for AI.”** These rules can be set at a global level in the app or on a per-project basis via files. There are two ways to provide project-specific rules to Cursor’s assistant:

1. A **project-level rules file named `.cursorrules`** in the repository root. This is essentially a plain text/Markdown file where you outline guidelines for the AI. Cursor will load this file when you open the project. It’s a simple approach (just one file containing all instructions for that project).

2. A more structured system of **multiple rule files under a `.cursor/rules/` directory**. Each rule file uses an `.mdc` extension (Markdown with frontmatter) and can be scoped or triggered in various ways. The frontmatter allows specifying if a rule is “Always” applied, or only for certain file patterns, or available for manual invocation, etc. Also, you can place `.cursor/rules` folders in subdirectories for hierarchical rule scoping (very similar to Codex’s nested `AGENTS.md` concept). For example, `frontend/.cursor/rules/rule.mdc` would apply to files in the frontend folder.

Under the hood, if using the `.cursor/rules` approach, Cursor likely concatenates all relevant rules (those marked “Always” and those matching open files) into the prompt it sends the model. If using the single `.cursorrules` file, it just uses that content globally.

For our project, we can actually use either. We might start with the simpler **`.cursorrules` root file** since SpeechDown isn’t huge. In this file, we’ll list out the guidelines in a straightforward way (much like we’d do in CLAUDE.md). Alternatively, if we want to take advantage of rule types, we could create separate files: e.g. `architecture.mdc` (Always include, describing layer rules), `testing.mdc` (maybe Auto-Attached when editing files under `tests/`), etc. This might be overkill for now, but demonstrates the flexibility.

**Defining Rules for Architecture & Naming:** We will ensure that our Cursor rules echo the content from other assistant configs. For instance, a `.cursorrules` file might contain:

* A brief architecture blurb: *“Use Ports and Adapters pattern: do not put external logic in domain; use interfaces in application layer and implementations in infrastructure. Keep presentation (CLI) separate from business logic.”*
* Naming rules: *“Ensure any new interface in `application/ports` is suffixed with Port and any new implementation in `infrastructure/adapters` is suffixed with Adapter. Follow existing examples in the codebase.”*
* Testing prompt: *“Whenever adding a feature, the AI should also generate corresponding tests under `tests/`. Tests should use pytest and the AAA pattern.”*

Cursor’s documentation suggests writing rules as **bullet points or short directives** – focused and actionable. We should avoid overly long prose. For example, rather than a paragraph, we might list:

* “Always keep domain logic free of I/O or external calls.”
* “Adapters (in infrastructure) must implement a Port interface from application.”
* “Name new domain classes with nouns (they represent entities or value objects).”
* “When writing tests, use pytest; put them in `tests/unit/` or `tests/integration/` as appropriate and use `assert` statements rather than print.”

These rules will be active in the Cursor IDE whenever the AI is invoked (either via chat or code generation commands). If we use `.mdc` files, we can refine triggers. For example, we could have an `architecture.mdc` marked as `Always` so it’s always in context. And maybe a `testing.mdc` marked as `Auto Attached` with a glob that matches `tests/**` so that when working on test files or when generating tests, those specific rules are considered. But given our rules aren’t huge, making them all always present is fine.

**Restricting Directory Modifications:** Cursor’s AI works by responding to user commands (like “Implement this function” or “Refactor this file”). It won’t roam around editing files on its own; it typically only modifies the open file or ones you direct it to. However, to avoid misplacement of code, our rules will remind it of where things belong. E.g.: *“If asked to add a new feature, determine which layer it should be in and suggest creating the file in that layer.”* Since Cursor’s AI is meant to assist the developer rather than operate fully autonomously, it might, for instance, tell the user “I will create a new file in `application/ports/` for the interface X and an implementation in `infrastructure/adapters/`.” The user then approves or asks Cursor to go ahead. Our rules serve to align its suggestions with the correct structure.

One interesting aspect: because Cursor rules can be scoped by directory, we can embed certain enforcement locally. For instance, in `domain/.cursor/rules/domain.mdc` we could put:

```md
---
description: Domain layer rules
alwaysApply: true
---
- Do not import from infrastructure or application layers here.
- Only use Python standard library or domain utilities.
```

Then, if the developer is editing a domain file and invokes Cursor, these rules are active and the AI will avoid suggesting an import that violates them. Similarly, in `infrastructure/.cursor/rules/infra.mdc` we might note: “This layer may depend on application and domain. All adapters here must correspond to a Port interface.” This localizes knowledge. For now, we can achieve a lot with the global `.cursorrules`, but this system gives us room to grow.

**Testing and Documentation:** We will include in the rules guidance like: “Don’t forget to update documentation (README or relevant docs) when adding features,” and “All code must have tests; prefer writing tests right after implementing a function.” In Cursor, a developer can actually multi-select files or instruct the AI across files, but still it’s user-driven. So, our rules act as an ever-present mentor in the IDE, nudging them. For example, if a dev uses Cursor’s “Implement function” command on a service function that hits a new port, Cursor might generate the function and also comment “(According to project rules, you should also add tests for this behavior).” Some community-shared Cursor rules even include patterns to always add a comment like “# TODO: add tests” if none detected – we could do something similar.

**Security Considerations:** Cursor’s AI (by default) uses either OpenAI’s models or their own, and code is sent to their server for analysis. This means any secret in open files or provided context could potentially be seen by the model. Unlike Copilot, Cursor doesn’t have an org-level policy to exclude files – it’s on the user to not feed secrets into a prompt. Our best mitigation is ensuring sensitive data isn’t in the repository or open editor tabs. We can also include a rule: *“Never include actual secret values in suggestions. If needing to show an example, use placeholder values.”* This reminds the AI not to output something that looks like a key. For path redaction, if we have any user-specific paths (unlikely in code), the underlying model might abstract them. (OpenAI’s models sometimes do that automatically, e.g., replacing personal directory names with generic ones in explanations – but it’s not guaranteed.) We haven’t encountered path leakage issues with Cursor; typically it’s working within the repo, so paths are relative anyway.

As for insecure dependencies, our rules can instruct the AI to prefer well-maintained libraries and the latest versions. But since Cursor is not deeply integrated with vulnerability databases, this is more of a general guideline. The developer’s own awareness (and tools like `pip audit`) are still needed. Cursor’s AI might occasionally suggest using a library; our rules could say “prefer standard library or existing dependencies unless absolutely necessary to add a new one.” This prevents the AI from introducing a random dependency that might be unvetted.

**Integration into Workflow:** Cursor is used by developers in their editor, so its rules don’t directly run in CI. The integration is about placing the `.cursorrules` file under version control so that everyone on the team with Cursor gets the same rules. We will commit this file (or the `.cursor/rules/` directory) to the repo. Team members using Cursor should enable “Include .cursorrules” in their Cursor settings (this might be on by default now). We will document in our contributor guide that if you use Cursor IDE, the project has curated rules to assist you. This ensures consistency: the AI in each developer’s environment sings from the same song sheet.

Furthermore, because this file is in the repo, if a new rule is needed (say we add a new convention), one can update `.cursorrules` and commit it, broadcasting the change to all. We might also share our rules publicly (anonymized if needed) since others have open-sourced their Cursor rules; but that’s optional. In CI, while Cursor itself isn’t present, we might implement a check that the `.cursorrules` file is not outdated (for example, we could write a script that compares `.cursorrules` content with ADR 008 to see if they diverge – though that’s manual for now). At minimum, we treat the rules file as documentation: if CI ensures ADR files have a certain structure, we could similarly ensure the rules file exists and perhaps lint it (e.g., no broken ADR reference links in it).

Finally, because Cursor rules are effectively another form of documentation for AI, we’ll keep them aligned with `AGENTS.md` and `CLAUDE.md`. It might be wise to consolidate these sources in one place (maybe generate them from a single template to avoid drift). In the implementation plan, we consider maintaining a master “AI-rules.md” that we then copy into each format as needed.

### Comparison of Assistant Capabilities

To summarize the differences and capabilities of each AI assistant with respect to rule configuration, we compiled the following comparison matrix:

| **Assistant**        | **Repo Rule File(s)**                                                                                         | **Rule Format & Scope**                                                                                                                                                                                                                                                                         | **Enforcement Strength**                                                                                                                                                                                                                                                                                                                        | **Security Features**                                                                                                                                                                                                                                                                                                                                                                | **CI/CD Integration**                                                                                                                                                                                                                                                                                   |
| -------------------- | ------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Google Jules**     | *No dedicated file.* (Prompt-based rules)                                                                     | N/A – Include guidelines in each task prompt. Issue descriptions serve as “instructions”. No persistent config in repo.                                                                                                                                                                         | Moderate – Relies on user to state rules every time. Jules will generally follow prompt instructions and presents a plan for approval, but no automatic checking beyond that.                                                                                                                                                                   | Isolated cloud VM per task (private execution). Doesn’t train on your code. Likely filters obvious sensitive data, but not much public info. User must manually avoid leaking secrets.                                                                                                                                                                                               | Trigger via GitHub Issue label or CLI. Can integrate by reviewing Jules’ PRs in CI (tests run on PR as usual). Community Actions exist to chain Jules with Copilot for reviews. No direct CI agent.                                                                                                     |
| **GitHub Copilot**   | `.github/copilot-instructions.md`; optional `*.prompt.md` files.                                              | Markdown text, any content. Instructions file applied globally to all Copilot Chat prompts for the repo. Prompt files are reusable task-specific chunks. No hierarchical scope (just one set of instructions for entire repo).                                                                  | Mild/Moderate – Influences AI suggestions (especially in Chat). Does not hard-prevent rule violations; depends on AI’s obedience. Developer must validate output. Inline code completions might ignore some context.                                                                                                                                 | Filters out known sensitive info and long code duplicates (to prevent leaking secrets or licensed code). Org settings allow excluding specific file paths from AI training/suggestions. Copilot can be set to block suggestions matching public code.                                                                                                                                | Primarily developer-side. However, Copilot for PRs can auto-summarize and review PRs, which can flag issues (leverages same instructions). We enable it for AI-assisted code reviews in CI (no code commit, just comments).                                                                             |
| **OpenAI Codex**     | `AGENTS.md` (at repo root and/or in subdirs).                                                                      | Markdown text, acts as an **agent playbook**. Hierarchical: file affects its directory subtree. Multiple `AGENTS.md` merge (nested overrides broader). Can include required commands/tests to run.                                                                                              | **Strong** – Codex is obligated to follow these instructions for any code it touches. It will run all tests/checks listed and iterate until they pass. Essentially serves as a set of unit tests for the AI’s output and process. Rules are effectively guaranteed unless overruled by user prompt.                                             | Runs in OpenAI cloud sandbox with ephemeral repo clone. Outputs are accompanied by log/test evidence. Likely scrubs or avoids direct leaking of any secret it sees. Internet access optional – can fetch patches or info securely if enabled.                                                                                                                                        | Accessible via ChatGPT UI or API/CLI. Not a persistent CI service, but can be invoked for complex tasks. After Codex-generated PRs, our normal CI runs. We can automate triggers (via API) in future. High assurance from built-in test enforcement reduces CI failures.                                |
| **Anthropic Claude** | `CLAUDE.md` (root, plus optional additional in subdirectories); also `.claude/settings.json` for permissions. | Markdown text, plain instructions. All `CLAUDE.md` files in or above current dir are pulled into context on start. No complex syntax needed (bullet lists encouraged). Settings JSON controls allowed/denied actions (not the AI’s logic, but its execution abilities).                         | Strong – Claude is highly obedient to written guidelines. It will consistently refer to `CLAUDE.md` and typically follow it verbatim unless instructed otherwise. However, it won’t self-verify by running tests unless asked (but our instructions can prompt it to do so). The agent’s design prioritizes following developer-provided rules. | Operates on user’s machine or GH Actions runner (code isn’t sent to Anthropic except via API calls). Emphasizes privacy (no training on our code by default). Harmlessness training means it avoids dangerous suggestions. We can restrict commands it runs (e.g., block networking) via policy files. In GH Action, it works only in response to mentions, and respects repo rules. | Used in CLI/IDE by developers with rules auto-loaded. Also has a GitHub Action for PRs/issues – easy setup to have Claude create or modify PRs on demand. We plan to use it for AI code reviews and possibly automated refactors. The `CLAUDE.md` ensures consistency in those automated contributions. |
| **Cursor IDE**       | `.cursorrules` (single file at root) *or* `.cursor/rules/` directory with multiple `*.mdc` rule files.        | `.cursorrules`: freeform text, global for project. `.mdc` files: Markdown with YAML header to define when to apply (Always, Auto (by glob), Manual, etc.). Can nest `.cursor/rules` in subdirectories for contextual rules. This allows fine granularity (e.g., rules only for files in `tests/`). | Moderate – Cursor will integrate the rules into the prompts it sends the model. The model (often GPT-4) will generally comply, especially for style or placement of code. But there’s no hard enforcement – it’s guidance used in generating suggestions. Developer oversight is needed to accept or reject changes.                            | No specialized secret filtering beyond the model’s base behavior. Code is sent to an API (OpenAI or local model) so sensitive data exposure is possible if not careful. Cursor doesn’t transmit excluded files unless opened. Security mainly comes from rule guidance (we instruct the AI not to do insecure things) and the developer’s own checks.                                | Developer IDE tool – not directly in CI. We include the rules file in the repo so all team members get the same AI behavior. No automated runs. However, rules can indirectly enforce standards by guiding developers, reducing rule violations before code even hits PR.                               |

In essence, OpenAI Codex and Anthropic Claude offer the most **forceful compliance** with repo rules (Codex by programmatically ensuring tests pass, Claude by extremely faithful adherence and configurable action restrictions). GitHub Copilot and Cursor provide **softer guardrails**, shaping AI suggestions but ultimately relying on the developer to enforce. Google Jules currently requires **manual instruction each time**, making it flexible but less consistent – though its plan/approve workflow provides a natural checkpoint. All tools benefit from our clearly defined rules, and none operates in isolation: we still have our CI tests and reviews as a safety net for any violations that slip through.

## Next Steps (Implementation Plan)

1.  **Establish a Master AI Rule File and Generation Process:** To ensure consistency and avoid drift between the various AI assistant rule files, we will implement a single source of truth:

    *   **Master Rule File (`docs/ai/AI-rules.md`):**
        *   A new file, `docs/ai/AI-rules.md`, will be created in the `docs/ai/` subdirectory to serve as the master document for all AI guidelines.
        *   This file will contain:
            *   Common sections for architectural principles (ADR 008), naming conventions, coding standards (Python version, linting with `ruff`, type checking with `mypy`), testing requirements (ADR 004: `pytest`, AAA pattern), and documentation practices (ADR 010: Design Docs, ADRs). This content will draw from existing ADRs and `pyproject.toml` settings.
            *   Agent-specific sections clearly delineated (e.g., using Markdown headings like `### For GitHub Copilot` or `### OpenAI Codex Specifics`). These sections will contain tailored instructions or phrasing suitable for each AI assistant, including how they should execute checks (e.g., `make ci` for **OpenAI Codex**).
        *   Example structure for `docs/ai/AI-rules.md`:
            ```markdown
            # Master AI Rules for SpeechDown

            ## Common Guidelines for All AI Assistants

            ### Architecture (ADR 008)
            - This project uses a Ports & Adapters architecture.
            - Domain layer (`src/speechdown/domain/`) must remain independent of external systems.
            - Application layer (`src/speechdown/application/`) defines interfaces (ports) in `application/ports/` and orchestrates use-cases in `application/services/`.
            - Infrastructure layer (`src/speechdown/infrastructure/`) provides implementations (adapters) of these ports in `infrastructure/adapters/`.
            - Presentation layer (`src/speechdown/presentation/`) handles CLI and output formatting.
            - Dependencies must point inward (e.g., infrastructure can depend on application/domain, but domain cannot depend on infrastructure).

            ### Naming Conventions
            - Interfaces: Suffix with `Port` (e.g., `TranscriptionPort`).
            - Adapters: Suffix with `Adapter` (e.g., `WhisperTranscriberAdapter`).
            - Services: Suffix with `Service` (e.g., `TranscriptionService`).

            ### Testing (ADR 004)
            - Use `pytest` for all tests.
            - Follow the Arrange-Act-Assert (AAA) pattern.
            - Place unit tests in `tests/unit/` and integration tests in `tests/integration/`, mirroring the `src/` structure.
            - All new code requires comprehensive tests.

            ### Coding Style & Quality
            - Python version >=3.11. Use f-strings and type hints.
            - Linting: `ruff` with a 100-character line length. Run `make lint` to check.
            - Type Checking: `mypy --strict`. Run `make mypy` to check.
            - All CI checks (`make ci`) must pass.

            ### Documentation (ADR 010)
            - Update relevant documentation for significant features.
            - Create Design Docs in `docs/design/current/` for new features.
            - Log architectural decisions as ADRs in `docs/adrs/current/`.

            ## Agent-Specific Instructions

            ### GitHub Copilot (`/.github/copilot-instructions.md`)
            # These instructions are added as hidden context to Copilot Chat.
            # Ensure Copilot is aware of the layered architecture and naming conventions.
            # Remind Copilot to suggest tests for new functionality.

            ### OpenAI Codex (`/AGENTS.md`)
            # Codex will merge instructions from all AGENTS.md files.
            # **Mandatory Checks for Codex:** "Before finalizing any code changes, you MUST execute the command `make ci` and ensure it passes without errors. If any test or check fails, you must fix the underlying issues in the code. Do not consider the task complete until `make ci` succeeds."

            ### Anthropic Claude (`/CLAUDE.md`)
            # Claude automatically includes CLAUDE.md content in its context.
            # **IMPORTANT for Claude:** "You MUST run `make ci` and confirm it passes before considering a task complete. Adhere strictly to the project's layered architecture and naming conventions when generating or modifying code."

            ### Cursor (`/.cursorrules`)
            # These rules guide Cursor's AI in the IDE.
            # - Domain logic must be free of I/O or external calls.
            # - Adapters in `infrastructure/` must implement a `Port` interface from `application/`.
            # - Always generate `pytest` tests for new functions or classes.
            ```
            *Note: The agent-specific sections in the example above use comments (`#`) or brief points as placeholders for more detailed, tailored instructions. In the actual `docs/ai/AI-rules.md`, these would be comprehensive Markdown instructions suitable for each AI.*

    *   **Generation Script (`scripts/generate_ai_rules.py`):**
        *   A Python script will be developed (e.g., `scripts/generate_ai_rules.py`) to parse the master `docs/ai/AI-rules.md` file.
        *   The script will identify common sections and agent-specific sections (e.g., by parsing Markdown headings or using custom markers if necessary).
        *   It will then generate the individual rule files in their respective locations:
            *   `.github/copilot-instructions.md`
            *   `AGENTS.md` (at the repository root)
            *   `CLAUDE.md` (at the repository root)
            *   `.cursorrules` (at the repository root)
        *   The script will ensure that each generated file contains the common guidelines plus its specific instructions, formatted appropriately for the target AI assistant.

    *   **Makefile Target (`generate-ai-rules`):**
        *   A new target will be added to the `Makefile` to automate the generation process:
            ```makefile
            .PHONY: generate-ai-rules
            generate-ai-rules:
            	@echo "Generating AI assistant rule files from master AI-rules.md..."
            	@python scripts/generate_ai_rules.py docs/ai/AI-rules.md
            	@echo "AI rule files generated successfully."
            ```
        *   This command (`make generate-ai-rules`) will be the standard way to update the agent-specific files after any changes to `docs/ai/AI-rules.md`. It is highly recommended to integrate this into a pre-commit hook to ensure generated files are always synchronized with the master file.

    *   **(No Jules-specific file to generate):** As **Google Jules** does not currently use a repository-based configuration file, we will continue to rely on a documented task template (e.g., `docs/ai/jules_task_template.md`). The content for this template should also be sourced from the common guidelines in `docs/ai/AI-rules.md` to maintain consistency in the instructions provided in Jules task prompts or issue descriptions. Any other AI-related prompt templates or documentation should also be placed in the `docs/ai/` subdirectory.

2.  **Align Rule Content with ADRs and PyProject:** Before finalizing those files, cross-check each against our ADRs and config:

    *   Ensure the **directory and layering definitions** in the files match ADR 008 exactly (so we don’t accidentally allow something ADR 008 forbids or vice versa). If ADR 008 is updated in the future, we must update these files accordingly.
    *   Include references to testing standards from ADR 004 (we might even quote “use pytest, functions over classes for tests” from it), and documentation process from ADR 010 (like reminding about design docs).
    *   Extract any relevant guidelines from `pyproject.toml` – e.g., the fact we use ruff, mypy, Python >=3.12 – and state them so the AI knows (for example, telling Copilot/Claude that f-strings are fine to use because we’re on 3.12, or telling Codex the exact mypy command).
    *   Mention our **type checking** explicitly since we have `mypy` in CI. The AI should generate proper type hints. (Codex and Claude especially can then run mypy and fix any typing issues automatically.)
    *   Verify no contradiction: e.g., if ADR or pyproject says one thing and we phrase rule differently, fix it. Consistency will prevent confusion for both AI and contributors.

3.  **Implement CI Checks (where feasible) for Rule Files:** While much of the enforcement is on the AI side, we can add a few CI steps to support this:

    *   Add a job in our GitHub Actions to **spell-check or lint the markdown** in these rule files (to avoid typos that could confuse the AI). Also check that any command or file path we mention actually exists (we could write a small script to parse out ` make ci` or `docs/adrs/008_project_architecture.md` references and verify they are valid). This prevents stale instructions. For example, if we rename a test command, this CI check will catch that the rule file still says the old command.
    *   We might also create a mapping of ADRs to these rules: e.g., ensure if ADR 008 is modified, a corresponding section in AGENTS.md/CLAUDE.md is present. This is harder to automate, but we could enforce manually via code review: any PR to ADRs or architecture should require updating AI rule files.
    *   As a simpler automated check, run `ruff` on the `*.md` files for consistency (ruff can lint Markdown too for style).
    *   Eventually, we could integrate a **dry-run AI check**: e.g., use OpenAI/Anthropic API in CI to analyze if our rule files have any obvious issues (“ hallucination test”). This is experimental – probably not needed immediately.

4.  **Introduce Tools Gradually and Gather Feedback:** Roll out the usage of these assistants one by one and refine:

    *   Once `copilot-instructions.md` is in place, enable Copilot’s repository instructions (it’s in public beta, so ensure all team members have updated extensions that support it). Solicit feedback: do they notice better suggestions or any odd behavior? Perhaps Copilot might become too verbose in chat by recalling our instructions – we might adjust wording if needed.
    *   When using OpenAI Codex (via ChatGPT interface), upload our repository and trigger a task. Observe how it uses `AGENTS.md`. We expect it to reference our instructions in its plan. If something is off (e.g., it misunderstood a rule), refine the wording. We may need to iterate on `AGENTS.md` with real tasks to ensure it’s effective.
    *   For Claude, have a developer use Claude CLI on a small feature with `CLAUDE.md` present. See if Claude follows the rules (it should perhaps even quote them sometimes like “As per the guidelines, I will do X.”). Any rule it ignores or misinterprets, we’ll adjust (maybe make it more prominent or simpler). Anthropic suggests not cramming too much – if our `CLAUDE.md` is too lengthy and some part isn’t being followed, we might break it into clearer, numbered rules or move less critical info to a lower section.
    *   Cursor rules can be tested by using Cursor IDE on a trivial change. Intentionally ask it something against the rules (e.g., “import requests in domain layer”) and see if it warns or avoids it. Since Cursor’s responses might not explicitly say “rule violation,” we gauge by whether it offers a compliant solution. If not, perhaps our rule phrasing needs to be stricter or the rule file might not be loaded (check that `.cursorrules` was picked up – in Cursor settings ensure the feature is on).

5.  **Security Validation:** To be extra sure, we’ll do a **dummy secret test**: place a fake secret in a test file (one that shouldn’t ever be output) and see if any AI suggests or exposes it. For instance, add `DUMMY_API_KEY = "12345"` in a config and ask Copilot or Claude about it. They should either ignore or pseudo-redact it. Copilot likely won’t produce it exactly due to training filter; Claude might respond carefully. This validates that the AIs are respecting their safety training. (After test, remove the dummy secret). Also test that our content exclusion works: if we added “secrets.json” to Copilot exclusion, try prompting Copilot with content from that file – it should refuse or not use it. Similarly, ensure our `AGENTS.md` test instructions indeed prevent Codex from completing tasks with failing tests (introduce a failing test deliberately and see if Codex stops to fix it). These tests ensure the guardrails actually function as expected in a safe manner.

6.  **Integrate AI usage into Developer Workflow:** With rules in place, update our contributor guide/documentation to instruct how to work with these assistants:

    *   Encourage the use of Copilot Chat with our guidelines file, and Copilot PR reviews, highlighting that the AI is tuned to our project.
    *   Provide guidance on using Codex or Claude for bulk changes – e.g. maybe have a `docs/ai/using-codex.md` where we document a successful run of Codex on the repo for a specific task (this can also serve as a case study for future).
    *   For Jules, document the standard prompt template (from `docs/ai/jules_task_template.md`) as mentioned. Perhaps automate a bit: we could create a GitHub issue template titled “Jules Task” that includes a section “**Jules Guidelines**: (Pre-filled with architecture & test reminders)”. So when someone opens a Jules task issue, they don’t forget to include those. This templating ensures even if another team member triggers Jules, the rules are there.
    *   For Cursor, let the team know the `.cursorrules` exists and what it does. Not everyone may use Cursor, but those who do should benefit.
    *   Emphasize that none of these rules remove the need for code review and CI checks; they are complementary. Developers should still understand the architecture – the AI is a helper, not an oracle. If an AI suggestion looks suspicious or off-pattern, question it. The presence of these files in the repo should also serve as a quick reference for new contributors about our standards (even if they’re not using the AI, reading `AGENTS.md` or `CLAUDE.md` gives a summary of how the project is structured).

7.  **Monitor and Iterate:** As we begin using the AIs with these configurations, we’ll monitor outcomes:

    *   Does code written with AI assistance pass CI more often on first try? Ideally yes, thanks to rules about running tests and lint fixes. If not, see what was missed (maybe add a rule for it).
    *   Are architecture violations dropping in code submissions? If we find PRs still showing, say, an Adapter not following naming or a util function in the wrong place, figure out if the AI contributed to that. If yes, perhaps refine the instructions to cover that scenario.
    *   Solicit feedback from developers: Are the AI responses helpful or are they being too verbose/restrictive? For example, Copilot might sometimes regurgitate our entire architecture blurb in a chat answer – if that noise becomes an issue, we might shorten the instructions or mark sections in HTML comments so Copilot sees them but doesn’t output them (a trick known to work sometimes).
    *   Keep an eye on maintainability: maintain the rule files as first-class artifacts. We will likely version them in tandem with ADR changes. For instance, if ADR 008 is superseded by a new architecture decision, update these files immediately in the same PR. Possibly even mention in ADRs that “AI assistant configs need update” to remind us.
    *   Also monitor if any rule causes undesired side-effects. E.g., if we said “always run tests,” maybe Copilot starts every suggestion with a comment about testing. If that happens, we might scope it better (perhaps such a line is only needed in Codex’s case where it actually runs them, not in Copilot’s instructions where it could be distracting).

8.  **Leverage Assistants for Compliance Enforcement:** Once the rules are solid, we can get creative in using the AIs to enforce rules on each other’s output:

    *   Use Claude or Codex to review existing code for any deviations from rules (basically an AI audit). They might find a few minor inconsistencies (which we can fix manually or via AI). This cleans up legacy issues so future AIs don’t get confused by past deviations.
    *   When Jules or Codex proposes a change, run Copilot Chat or Claude on that diff to see if it spots issues. For example, paste a Jules diff into Claude and ask “Does this follow our project guidelines?” Since Claude has CLAUDE.md loaded, it can answer if something stands out. This is a form of AI-on-AI double-check and could be integrated manually in reviews.
    *   Perhaps automate Claude review: since we have a Claude Action, we could trigger it on Jules’ PR with a prompt like “Check adherence to guidelines.” This would give us an automated comment if Jules missed something.
    *   Over time, if these cross-checks prove reliable, we could formalize them in CI (e.g., an action that blocks merge if Claude finds a serious rule violation). But we’ll start with advisory mode to build trust in the system.

By implementing these steps, we expect AI-generated code contributions to seamlessly comply with SpeechDown’s architecture and quality standards. The combination of clearly defined rules (drawn from our ADRs) and the enforcement capabilities of each platform will reduce the oversight burden on human reviewers and prevent architecture “drift” caused by AI suggestions. In the long run, this should speed up development (AI can handle boilerplate while following best practices) and improve consistency (every contributor, human or AI, is guided by the same rule-set). We will remain vigilant and update the rules configuration as both the project and the AI tools evolve. Each assistant’s ecosystem is rapidly improving – for example, Copilot might introduce more granular controls, or Jules might allow a config file in future – and we will adapt our approach to leverage new features that strengthen our AI guardrails. With this plan in place, SpeechDown can confidently embrace AI assistance, knowing that our domain-driven design and other standards are being actively safeguarded throughout the development process.

